{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/","height":84,"referenced_widgets":["30d15e5d64d742b6b45b3d7061794abb","ef514320434f4b73a1eadcb4b0c80099","385b60ba409142e89fa3f819db302e03","1f62af2c63fe4452943708ac17cbdb3d","4e7bcc7fe4be4e11882710d653cf2051","f74e041e627742e5b46f95eb6d6d93be","d1466ae0e0604ffc810efbb11f09833e","f63010e1d0c745ca8c8d9992bdac7834","04d280838e914335904501d997613af7","c788ae61afe641339e97d5175e572d10","68cd5669e36a4ec0a5ea846168fd506e"]},"id":"MD6iQcqtzKX0","executionInfo":{"status":"ok","timestamp":1650461836364,"user_tz":-420,"elapsed":2966,"user":{"displayName":"Xuân Trường","userId":"08106464541113705100"}},"outputId":"aa26bf09-9049-40e3-a80a-f717cb7049fe"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"]},{"output_type":"display_data","data":{"text/plain":["0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30d15e5d64d742b6b45b3d7061794abb"}},"metadata":{}}],"source":["import string\n","import numpy as np\n","from PIL import Image\n","import os\n","from pickle import dump, load\n","import numpy as np\n","\n","import tensorflow as tf\n","\n","from keras.applications.xception import Xception, preprocess_input\n","from keras.preprocessing.image import load_img, img_to_array\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","from keras.layers.merge import add\n","from keras.models import Model, load_model\n","from keras.layers import Input, Dense, LSTM, Embedding, Dropout\n","\n","# small library for seeing the progress of loops.\n","from tqdm import tqdm_notebook as tqdm\n","tqdm().pandas()"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"FZ2RZuLczKX9"},"outputs":[],"source":["# Loading a text file into memory\n","def load_doc(filename):\n","    # Opening the file as read only\n","    file = open(filename, 'r')\n","    text = file.read()\n","    file.close()\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"3Lgocze0zKYA"},"outputs":[],"source":["# get all imgs with their captions\n","def all_img_captions(filename):\n","    file = load_doc(filename)\n","    captions = file.split('\\n')\n","    descriptions ={}\n","    for caption in captions[:-1]:\n","        img, caption = caption.split('\\t')\n","        if img[:-2] not in descriptions:\n","            descriptions[img[:-2]] = [caption]\n","        else:\n","            descriptions[img[:-2]].append(caption)\n","    return descriptions"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"VQ_Bt5IczKYD"},"outputs":[],"source":["##Data cleaning- lower casing, removing puntuations and words containing numbers\n","def cleaning_text(captions):\n","    table = str.maketrans('','',string.punctuation)\n","    for img,caps in captions.items():\n","        for i,img_caption in enumerate(caps):\n","\n","            img_caption.replace(\"-\",\" \")\n","            desc = img_caption.split()\n","\n","            #converts to lower case\n","            desc = [word.lower() for word in desc]\n","            #remove punctuation from each token\n","            desc = [word.translate(table) for word in desc]\n","            #remove hanging 's and a \n","            desc = [word for word in desc if(len(word)>1)]\n","            #remove tokens with numbers in them\n","            desc = [word for word in desc if(word.isalpha())]\n","            #convert back to string\n","\n","            img_caption = ' '.join(desc)\n","            captions[img][i]= img_caption\n","    return captions"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"Qwp_R4GfzKYH"},"outputs":[],"source":["def text_vocabulary(descriptions):\n","    # build vocabulary of all unique words\n","    vocab = set()\n","    \n","    for key in descriptions.keys():\n","        [vocab.update(d.split()) for d in descriptions[key]]\n","    \n","    return vocab"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"cAXCQEAPzKYJ"},"outputs":[],"source":["#All descriptions in one file \n","def save_descriptions(descriptions, filename):\n","    lines = list()\n","    for key, desc_list in descriptions.items():\n","        for desc in desc_list:\n","            lines.append(key + '\\t' + desc )\n","    data = \"\\n\".join(lines)\n","    file = open(filename,\"w\")\n","    file.write(data)\n","    file.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"IH1-CZlhzKYM"},"outputs":[],"source":["# all_train_captions = []\n","# for key, val in descriptions.items():\n","#     for cap in val:\n","#         all_train_captions.append(cap)\n","\n","# # Consider only words which occur at least 8 times in the corpus\n","# word_count_threshold = 8\n","# word_counts = {}\n","# nsents = 0\n","# for sent in all_train_captions:\n","#     nsents += 1\n","#     for w in sent.split(' '):\n","#         word_counts[w] = word_counts.get(w, 0) + 1\n","\n","# vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n","\n","# print('preprocessed words %d ' % len(vocab))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"8nXdvhe1zKYQ"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"vY_oXk1uzKYS"},"outputs":[],"source":["dataset_text = \"/content/drive/MyDrive/Image Caption Generator/Dataset/Flickr_8k_text\"\n","dataset_images = \"/content/drive/MyDrive/Image Caption Generator/Dataset/Flicker8k_Dataset\""]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/"},"id":"IdN5lwMJzKYU","executionInfo":{"status":"ok","timestamp":1650461869011,"user_tz":-420,"elapsed":1226,"user":{"displayName":"Xuân Trường","userId":"08106464541113705100"}},"outputId":"3cbc9856-9735-4525-8a11-2da7c7b4db41"},"outputs":[{"output_type":"stream","name":"stdout","text":["Length of descriptions = 8092\n","Length of vocabulary =  8763\n"]}],"source":["#we prepare our text data\n","filename = dataset_text + \"/\" + \"Flickr8k.token.txt\"\n","#loading the file that contains all data\n","#mapping them into descriptions dictionary img to 5 captions\n","descriptions = all_img_captions(filename)\n","print(\"Length of descriptions =\" ,len(descriptions))\n","\n","#cleaning the descriptions\n","clean_descriptions = cleaning_text(descriptions)\n","\n","#building vocabulary \n","vocabulary = text_vocabulary(clean_descriptions)\n","print(\"Length of vocabulary = \", len(vocabulary))\n","\n","#saving each description to file \n","save_descriptions(clean_descriptions, \"descriptions.txt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"9rzsmYJezKYX"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"r9C1qqZ0zKYY"},"outputs":[],"source":["\n","def extract_features(directory):\n","        model = Xception( include_top=False, pooling='avg' )\n","        features = {}\n","        for img in tqdm(os.listdir(directory)):\n","            filename = directory + \"/\" + img\n","            image = Image.open(filename)\n","            image = image.resize((299,299))\n","            image = np.expand_dims(image, axis=0)\n","            #image = preprocess_input(image)\n","            image = image/127.5\n","            image = image - 1.0\n","            \n","            feature = model.predict(image)\n","            features[img] = feature\n","        return features"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"pycharm":{"name":"#%%\n"},"id":"qKKf_7mqzKYZ"},"outputs":[],"source":["#2048 feature vector\n","features = extract_features(dataset_images)\n","dump(features, open(\"features.p\",\"wb\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"pycharm":{"name":"#%%\n"},"id":"QLOk2uZgzKYa"},"outputs":[],"source":["features = load(open(\"/content/drive/MyDrive/Image Caption Generator/features.p\",\"rb\"))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"0KwSe9DqzKYb"},"outputs":[],"source":["#load the data \n","def load_photos(filename):\n","    file = load_doc(filename)\n","    photos = file.split(\"\\n\")[:-1]\n","    return photos\n","\n","\n","def load_clean_descriptions(filename, photos):   \n","    #loading clean_descriptions\n","    file = load_doc(filename)\n","    descriptions = {}\n","    for line in file.split(\"\\n\"):\n","        \n","        words = line.split()\n","        if len(words)<1 :\n","            continue\n","    \n","        image, image_caption = words[0], words[1:]\n","        \n","        if image in photos:\n","            if image not in descriptions:\n","                descriptions[image] = []\n","            desc = '<start> ' + \" \".join(image_caption) + ' <end>'\n","            descriptions[image].append(desc)\n","\n","    return descriptions\n","\n","\n","def load_features(photos):\n","    #loading all features\n","    all_features = load(open(\"/content/drive/MyDrive/Image Caption Generator/features.p\",\"rb\"))\n","    #selecting only needed features\n","    features = {k:all_features[k] for k in photos}\n","    return features\n"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"DTtNz2EizKYb"},"outputs":[],"source":["filename = dataset_text + \"/\" + \"Flickr_8k.trainImages.txt\"\n","\n","#train = loading_data(filename)\n","train_imgs = load_photos(filename)\n","train_descriptions = load_clean_descriptions(\"/content/drive/MyDrive/Image Caption Generator/descriptions.txt\", train_imgs)\n","train_features = load_features(train_imgs)"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"nkNHY_FHzKYc"},"outputs":[],"source":["#converting dictionary to clean list of descriptions\n","def dict_to_list(descriptions):\n","    all_desc = []\n","    for key in descriptions.keys():\n","        [all_desc.append(d) for d in descriptions[key]]\n","    return all_desc\n","\n","#creating tokenizer class \n","#this will vectorise text corpus\n","#each integer will represent token in dictionary \n","\n","from keras.preprocessing.text import Tokenizer\n","\n","def create_tokenizer(descriptions):\n","    desc_list = dict_to_list(descriptions)\n","    tokenizer = Tokenizer()\n","    tokenizer.fit_on_texts(desc_list)\n","    return tokenizer\n"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/"},"id":"J09Id3KszKYc","executionInfo":{"status":"ok","timestamp":1650461976411,"user_tz":-420,"elapsed":1018,"user":{"displayName":"Xuân Trường","userId":"08106464541113705100"}},"outputId":"ee68b93d-fadf-4db7-aa86-2662afdcde9d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["7577"]},"metadata":{},"execution_count":17}],"source":["# give each word a index, and store that into tokenizer.p pickle file\n","tokenizer = create_tokenizer(train_descriptions)\n","dump(tokenizer, open('/content/drive/MyDrive/Image Caption Generator/tokenizer.p', 'wb'))\n","vocab_size = len(tokenizer.word_index) + 1\n","vocab_size "]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/"},"id":"c7NUW2x3zKYd","executionInfo":{"status":"ok","timestamp":1650461980000,"user_tz":-420,"elapsed":443,"user":{"displayName":"Xuân Trường","userId":"08106464541113705100"}},"outputId":"2555fc6b-4603-431c-fc84-eb74559865bd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["32"]},"metadata":{},"execution_count":18}],"source":["#calculate maximum length of descriptions\n","def max_length(descriptions):\n","    desc_list = dict_to_list(descriptions)\n","    return max(len(d.split()) for d in desc_list)\n","\n","max_length = max_length(descriptions)\n","max_length"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"oVMxbc5azKYd","outputId":"f8e4d216-ec03-4fd4-f5c6-43207799789e"},"outputs":[{"data":{"text/plain":["array([0.36452794, 0.12713662, 0.0013574 , ..., 0.221817  , 0.01178991,\n","       0.24176797], dtype=float32)"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["##features['1000268201_693b08cb0e.jpg'][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"w5OYE7SazKYe"},"outputs":[],"source":["# Define the model\n","\n","#1 Photo feature extractor - we extracted features from pretrained model Xception. \n","#2 Sequence processor - word embedding layer that handles text, followed by LSTM \n","#3 Decoder - Both 1 and 2 model produce fixed length vector. They are merged together and processed by dense layer to make final prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"eh6Z6oo6zKYf"},"outputs":[],"source":["#create input-output sequence pairs from the image description.\n","\n","#data generator, used by model.fit_generator()\n","def data_generator(descriptions, features, tokenizer, max_length):\n","    while 1:\n","        for key, description_list in descriptions.items():\n","            #retrieve photo features\n","            feature = features[key][0]\n","            input_image, input_sequence, output_word = create_sequences(tokenizer, max_length, description_list, feature)\n","            yield [[input_image, input_sequence], output_word]         \n","\n","def create_sequences(tokenizer, max_length, desc_list, feature):\n","    X1, X2, y = list(), list(), list()\n","    # walk through each description for the image\n","    for desc in desc_list:\n","        # encode the sequence\n","        seq = tokenizer.texts_to_sequences([desc])[0]\n","        # split one sequence into multiple X,y pairs\n","        for i in range(1, len(seq)):\n","            # split into input and output pair\n","            in_seq, out_seq = seq[:i], seq[i]\n","            # pad input sequence\n","            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n","            # encode output sequence\n","            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n","            # store\n","            X1.append(feature)\n","            X2.append(in_seq)\n","            y.append(out_seq)\n","    return np.array(X1), np.array(X2), np.array(y)"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/"},"id":"cMbmaLiQzKYf","executionInfo":{"status":"ok","timestamp":1650462006490,"user_tz":-420,"elapsed":6,"user":{"displayName":"Xuân Trường","userId":"08106464541113705100"}},"outputId":"2ba3afb4-dd33-4e1d-9dc5-fca7f2374e69"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((47, 2048), (47, 32), (47, 7577))"]},"metadata":{},"execution_count":20}],"source":["[a,b],c = next(data_generator(train_descriptions, features, tokenizer, max_length))\n","a.shape, b.shape, c.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"YBFzf4ZQzKYg"},"outputs":[],"source":["from tensorflow.keras.utils import plot_model\n","\n","# define the captioning model\n","def define_model(vocab_size, max_length):\n","    \n","    # features from the CNN model squeezed from 2048 to 256 nodes\n","    inputs1 = Input(shape=(2048,))\n","    fe1 = Dropout(0.5)(inputs1)\n","    fe2 = Dense(256, activation='relu')(fe1)\n","\n","    # LSTM sequence model\n","    inputs2 = Input(shape=(max_length,))\n","    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n","    se2 = Dropout(0.5)(se1)\n","    se3 = LSTM(256)(se2)\n","\n","    # Merging both models\n","    decoder1 = add([fe2, se3])\n","    decoder2 = Dense(256, activation='relu')(decoder1)\n","    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n","    \n","    # tie it together [image, seq] [word]\n","    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n","    model.compile(loss='categorical_crossentropy', optimizer='adam')\n","    \n","    # summarize model\n","    print(model.summary())\n","    plot_model(model, to_file='model.png', show_shapes=True)\n","    \n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/"},"id":"RxaSef4GzKYg","executionInfo":{"status":"ok","timestamp":1650467896343,"user_tz":-420,"elapsed":2309800,"user":{"displayName":"Xuân Trường","userId":"08106464541113705100"}},"outputId":"d83cd675-dd13-41af-f406-ce7dc96bfd50"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Dataset:  6000\n","Descriptions: train= 6000\n","Photos: train= 6000\n","Vocabulary Size: 7577\n","Description Length:  32\n","Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_3 (InputLayer)           [(None, 32)]         0           []                               \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, 2048)]       0           []                               \n","                                                                                                  \n"," embedding (Embedding)          (None, 32, 256)      1939712     ['input_3[0][0]']                \n","                                                                                                  \n"," dropout (Dropout)              (None, 2048)         0           ['input_2[0][0]']                \n","                                                                                                  \n"," dropout_1 (Dropout)            (None, 32, 256)      0           ['embedding[0][0]']              \n","                                                                                                  \n"," dense (Dense)                  (None, 256)          524544      ['dropout[0][0]']                \n","                                                                                                  \n"," lstm (LSTM)                    (None, 256)          525312      ['dropout_1[0][0]']              \n","                                                                                                  \n"," add_12 (Add)                   (None, 256)          0           ['dense[0][0]',                  \n","                                                                  'lstm[0][0]']                   \n","                                                                                                  \n"," dense_1 (Dense)                (None, 256)          65792       ['add_12[0][0]']                 \n","                                                                                                  \n"," dense_2 (Dense)                (None, 7577)         1947289     ['dense_1[0][0]']                \n","                                                                                                  \n","==================================================================================================\n","Total params: 5,002,649\n","Trainable params: 5,002,649\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  from ipykernel import kernelapp as app\n"]},{"output_type":"stream","name":"stdout","text":["6000/6000 [==============================] - 585s 97ms/step - loss: 4.5112\n","6000/6000 [==============================] - 579s 97ms/step - loss: 3.6690\n","6000/6000 [==============================] - 576s 96ms/step - loss: 3.3801\n","6000/6000 [==============================] - 577s 96ms/step - loss: 3.2072\n","6000/6000 [==============================] - 572s 95ms/step - loss: 3.0861\n","6000/6000 [==============================] - 571s 95ms/step - loss: 3.0000\n","6000/6000 [==============================] - 574s 96ms/step - loss: 2.9299\n","6000/6000 [==============================] - 570s 95ms/step - loss: 2.8730\n","6000/6000 [==============================] - 571s 95ms/step - loss: 2.8296\n","6000/6000 [==============================] - 570s 95ms/step - loss: 2.7927\n"]}],"source":["# train our model\n","print('Dataset: ', len(train_imgs))\n","print('Descriptions: train=', len(train_descriptions))\n","print('Photos: train=', len(train_features))\n","print('Vocabulary Size:', vocab_size)\n","print('Description Length: ', max_length)\n","\n","model = define_model(vocab_size, max_length)\n","epochs = 10\n","steps = len(train_descriptions)\n","# making a directory models to save our models\n","os.mkdir(\"models\")\n","for i in range(epochs):\n","    generator = data_generator(train_descriptions, train_features, tokenizer, max_length)\n","    model.fit_generator(generator, epochs=1, steps_per_epoch= steps, verbose=1)\n","    model.save(\"models/model_\" + str(i) + \".h5\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"},"colab":{"name":"training_caption_generator.ipynb","provenance":[],"collapsed_sections":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"30d15e5d64d742b6b45b3d7061794abb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ef514320434f4b73a1eadcb4b0c80099","IPY_MODEL_385b60ba409142e89fa3f819db302e03","IPY_MODEL_1f62af2c63fe4452943708ac17cbdb3d"],"layout":"IPY_MODEL_4e7bcc7fe4be4e11882710d653cf2051"}},"ef514320434f4b73a1eadcb4b0c80099":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f74e041e627742e5b46f95eb6d6d93be","placeholder":"​","style":"IPY_MODEL_d1466ae0e0604ffc810efbb11f09833e","value":""}},"385b60ba409142e89fa3f819db302e03":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f63010e1d0c745ca8c8d9992bdac7834","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_04d280838e914335904501d997613af7","value":0}},"1f62af2c63fe4452943708ac17cbdb3d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c788ae61afe641339e97d5175e572d10","placeholder":"​","style":"IPY_MODEL_68cd5669e36a4ec0a5ea846168fd506e","value":" 0/? [02:59&lt;?, ?it/s]"}},"4e7bcc7fe4be4e11882710d653cf2051":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f74e041e627742e5b46f95eb6d6d93be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1466ae0e0604ffc810efbb11f09833e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f63010e1d0c745ca8c8d9992bdac7834":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"04d280838e914335904501d997613af7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c788ae61afe641339e97d5175e572d10":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68cd5669e36a4ec0a5ea846168fd506e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}